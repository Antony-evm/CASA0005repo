# Descriptive statistics 

# Learning objectives

By the end of this practical you should be able to:

1. Create descriptive plots (histograms and boxplots) to help understand the frequency distributions of your data
1. Write custom functions to process your data
1. Produce a location quotient map to highlight interesting (above and below average) patterns in your data
1. Write a function in R to produce a range of different maps based on user inputs
1. Perform a very basic cluster analysis and output the results of a basic geodemographic classification

# Homework

Outside of our schedulded sessions you should be doing around 12 hours of extra study per week. Feel free to follow your own GIS interests, but good places to start include the following:

::: {.infobox .assignment data-latex="{note}"}

**Assignment**

From weeks 6-9, learn and practice analysis from the course and identify appropriate techniques (from wider research) that might be applicable/relevant to your data. Conduct an extensive methodological review – this could include analysis from within academic literature and/or government departments (or any reputable source).
:::

::: {.infobox .note data-latex="{note}"}
**Reading**

* For k-means clustering and exploratory data analysis read [Chapter 12 "K-Means Clustering"](https://bookdown.org/rdpeng/exdata/k-means-clustering.html) from Exploratory Data Analysis with R by Peng (2016).

Remember this is just a starting point, explore the [reading list](https://rl.talis.com/3/ucl/lists/139FBAF8-DACD-60FB-8BDC-E9C9E09BA885.html?lang=en-GB&login=1), practical and lecture for more ideas.
:::

## Recommended listening `r emo::ji("headphones")`

Some of these practicals are long, take regular breaks and have a listen to some of our fav tunes each week.

[Andy](https://www.youtube.com/watch?v=6Bh6IL1mNfc)


[Adam](https://open.spotify.com/track/3BGF4XAm8jaEi2LcGR57O3?si=gfYc6LS6Tgy9M9oXVVG4SQ)
Bit different from me this week - I'm not all about drum and bass (although I mostly am). Stumbled across these recently and I should have known about them for much longer. The Joy Formidable - this is a 10th Anniversary revisit of their first EP and is ace. If you like Welsh, you're in for a treat! 


# Getting Started
Before we begin this week’s practical, we need to load our packages and carry out some data preparation... 

```{r prac8_load, message=FALSE}
library(highcharter)
library(tidyverse)
library(downloader)
library(rgdal)
library(sf)
library(ggplot2)
library(reshape2)
library(plotly)
library(raster)
library(downloader)
library(rgdal)
```


There is a problem with our London Wards data --- we are missing some data relating to housing tenure. The housing tenure data in this file comes from the 2011 Census and visiting http://www.nomisweb.co.uk/ and interrogating Table KS402EW (the Tenure table), we can discover that data for the percentage of shared owners and those living in accommodation rent free are missing. 

Rather than making you go off to Nomisweb and fetch this data, because I'm really nice, I've posted on GitHub a file containing this and extra categorical, ratio and geographical data that we will need to add to our existing London data file. To download this consult [How to download data and files from GitHub], i'd used Option 1 and it's the prac8_data folder you want to download.

We can easily join this new data to our original data in R.

```{r prac8_read_data}

LondonWards <- st_read(here::here("prac8_data", 
                                  "New_ward_data",
                                  "NewLondonWard.shp"))

extradata <- read_csv(here::here("prac8_data", "LondonAdditionalDataFixed.csv"))

LondonWardsleftjoin <- LondonWards %>%
  left_join(.,extradata,
            by = c("WD11CD" = "Wardcode"))

#LondonWardsSF <- merge(LondonWards, extradata, by.x = "WD11CD", by.y = "Wardcode")
```

# Main Tasks

## Task 1 - Descriptive Statistics
Using the lecture notes for guidance, you should generate the following graphs and descriptive statistics using standard functions and ggplot2 in R. Each element should be copied and saved to a word document or something similar:

Generate the following from your `LondonWardsSF` data frame:

* A simple histogram for a scale/ratio variable of your choice
* A simple histogram for a scale/ratio variable of your with a different frequency bin-width
    + The same histogram with vertical lines for the mean, median and mode (the mode will be the mid value for the bin with the largest count) and the inter-quartile range. *hint – use summary(table$variable) to find the values if you are not sure*
    + The same histogram with three different kernel density smoothed frequency gradients
* A boxplot of the same variable
* A faceted grid of histograms with for every variable in your London Wards data file. In order to do this, you will need to remove Factor (non-numeric) variables from your dataset and re-shape your data using the `melt()` function in the `reshape2` package (hint – check the help file for `melt.data.frame()` to understand what the code below is doing). The code below will help you:

```{r prac8_data_manipulate, message=FALSE, warning=FALSE}
#check which variables are numeric first

Datatypelist <- LondonWardsleftjoin %>% 
  st_drop_geometry()%>%
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

#make groups based on types of variables
Groups <- LondonWardsleftjoin %>% 
  st_drop_geometry()%>%
  dplyr::select(is.numeric)%>%
  pivot_longer(everything(),
               names_to="All_variables", 
               values_to="val")%>%
  mutate(All_variables = tolower(All_variables))%>%
  mutate(group = case_when(str_detect(All_variables, "age") ~ "Age",
                           str_detect(All_variables, "employ|income|job|jsa") ~ "Employment",
                           str_detect(All_variables,
                                      "house|rent|detatched|flat|terrace|owned|social|private|share|tax|mortgage") ~ "Housing", TRUE~"Other"))

Agehist <- Groups%>%
  filter(group=="Age")%>%
  ggplot(., aes(x=val)) + 
  geom_histogram(aes(x = val, y = ..density..))+
  geom_density(colour="red", size=1, adjust=1)+
  facet_wrap(~All_variables, scales = 'free')

```

To change which group we are plotting, simply alter the `filter()` argument.

Make a note of which variables appear normally distributed and which appear to be skewed. What do the histograms for nominal and ordinal data look like?

Try performing a log10() transformation on the x variables and plotting a similar facet grid of histograms –-- what does this do to some of the skewed variables? For example...

```{r prac8_hist}

Agehist <- Groups%>%
  filter(group=="Age")%>%
  ggplot(., aes(x=log10(val))) + 
  geom_histogram(aes(x = log10(val), y = ..density..))+
  geom_density(colour="red", size=1, adjust=1)+
  facet_wrap(~All_variables, scales = 'free')

```

* Create a 2D histogram and 2D kernel density estimate of ward centroids in London using the Eastings and Northings data in the x and y columns of your dataset. For example:

```{r prac8_ggplot}
Londonpoint <- ggplot(LondonWardsleftjoin, 
                      aes(x=x.y,y=y.y))+
  geom_point()+
  coord_equal()

Londonpoint

Londonpoint<-ggplot(LondonWardsleftjoin, 
                    aes(x=x.y,y=y.y))+
  stat_bin2d(bins=10)

Londonpoint

Londonpoint<-ggplot(LondonWardsleftjoin, aes(x=x.y,y=y.y))+geom_point()+coord_equal()
Londonpoint

Londonpoint+stat_density2d(aes(fill = ..level..), geom="polygon")

```

## Extension 1

If you really want to go down the road of carrying out KDE in a range of different ways, then this [Kernel Density Estimation tutorial](http://egallic.fr/R/sKDE/smooth-maps/kde.html) --- perhaps try it with some of the Blue Plaques data from previous weeks. 

## Task 2 - Function to recode data

In the lecture, it was mentioned that sometimes we should recode variables to reduce the amount of information contained in order that different tests can be carried out on the data. Here we will recode some of our scale/ratio data into some nominal/weak-ordinal data to carry out some basic analysis on. 

A function to recode data in our dataset might look like the one below:

```{r, prac8_fun2, eval=T}
newvar<-0
recode<-function(variable,high,medium,low){
  newvar[variable<=high]<-"High"
  newvar[variable<=medium]<-"Medium"
  newvar[variable<=low]<-"Low"
  return(newvar)
}
```

## What's going on in this function?

* First we initialise a new variable called `newvar` and set it to = 0. We then define a new function called `recode`. This takes in 4 pieces of information: A variable (called `variable` but I could have called it anything) and three values called `high`, `medium` and `low`. It outputs a value to the new string variable `newvar` based on the values of high, medium and low that are given to the function. 
	
* To create the function in R, highlight the all of the code in the function and then run the whole block (ctrl-Return in R-Studio). You will see that the function is stored in the workspace.
	
* We can now use this function to recode any of our continuous variables into high, medium and low values based on the values we enter into the function. 

* We are going to recode the Average GCSE Score variable into High, Medium and Low values – High will be anything above the 3rd Quartile, Low will be anything below the 1st Quartile and Medium – anything in between.

*Note, if your data doesn't have the 2013 GCSE scores but 2014, it will have different figures to these figures below and you will need to call the column by the column header you have*

```{r prac8_attatch, message=FALSE, warning=FALSE}
attach(LondonWards)
#Check the name of your column, there could be a slight error and it might be called 'AvgGCSED201'
summary(AvgGCSE201) 
```

Create a new column in your data frame and fill it with recoded data for the Average GCSE Score in 2013. To do this, pass the AvgGCSE2013 variable to the `recode()` function, along with and the three values for high, medium and low. You should create a new variable called gcse_recode and use the function to fill it with values

If you wanted to be really fancy, you could try altering the function to calculate these “High”, “Medium” and “Low”

```{r prac8_recode}
LondonWards <- LondonWards %>%
  mutate(GCSE_recode = recode(AvgGCSE201,409.1,358.3,332.3))

#or

LondonWards$GCSE_recode <- recode(AvgGCSE201,409.1,358.3,332.3)
```

You should also create a second re-coded variable from the unauthorised absence variable using the same function – call this variable `unauth_recode` and again, used the 3rd and 1st quartiles to define your high, medium and low values.

On to another function. This time, we will calculate some location quotients for housing tenure in London. If you remember, a location quotient is simply the ratio of a local distribution to the ratio of a global distribution. In our case, our global distribution will be London. 

```{r prac8_LQ}
#Location Quotient function 1
LQ1<-function(pctVariable){
  pctVariable /mean(pctVariable)
}
#Location Quotient function 2
LQ2<-function(variable,rowtotal){
  localprop<-variable/rowtotal
  globalprop<-sum(variable)/sum(rowtotal)
  return(localprop/globalprop)
}
```

The two functions above calculate the same Location Quotient, but the first one works on variables which have already been converted into row percentages, the second will work on raw variables where an additional column for the row totals is stored in a separate column – e.g. “age 0-15”, “age 16-64” and “age 65 plus” all sum to the “Pop2013” column in our data London Wards data set:

```{r prac8_head}
head(LondonWards[,1:7])
```

Calculate Location Quotients for the 5 Housing tenure variables (Owner Occupied, Private Rent, Social Rent, Shared Ownership, Rent Free) in your data set using either of the functions above. Save these as 5 new variables in your dataset. *Hint – use the function to create the variable directly, for example:

```{r prac8_df, eval=FALSE}
#this is pseudo code, but you should see how this works
dataframe$newLQVariable <- LQ1(originalPercentageVariable)
#or
dataframe$newLQVariable <- LQ2(originalVariable,rowTotalVariable)
```

```{r prac8_cols, include=FALSE}
#note, you will probably need to change the column headers of the columns you are referring to here
attach(LondonWards)
summary(LondonWards)
LondonWards$unauth_recode <- recode(UnauthAbse,2.4657,1.4105,0.8215) 
LondonWards$LQOwned <- LQ1(PctOwned20)
LondonWards$LQSocRe <- LQ1(PctSocialR)
LondonWards$LQPriRe <- LQ1(PctPrivate)
LondonWards$LQShare <- LQ1(PctSharedO)
LondonWards$LQRentF <- LQ1(PctRentFre)
```

## Task 3 – Mapping Location Quotients

You should now try and create a map or series of maps of your housing tenure location quotients using `tmap` or `ggplot`. Try to create a map by referring back earlier practicals in this course and follow the steps from there (or, indeed, use your memory)

## Task 4 – Creating a Basic Geodemographic Classification

As we saw in the lecture, geodemographic classifications are widely used to classify areas according to the characteristics of the population that inhabits them. All geodemographic classifications are created using cluster analysis algorithms. Many of these algorithms exist, but one of the most commonly used is k-means.

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_1.jpg')
```

One of the pitfalls of these algorithms is that they will always find a solution, whether the variables have been selected appropriately or standardised correctly. This means that it’s very easy to create a classification which is misleading. 

All of that said, it is useful to see how straightforward it is to create a classification yourself to describe some spatial data you have. 

In a cluster analysis, you should select variables that are:

* Ranged on the same scale

* Normally distributed

* Not highly correlated

To make this task easier, we will just select two variables to make our classification from. In a real geodemographic classification, hundreds of variables are often used. 

```{r prac8_cluster}
LondonWardsData <- LondonWards %>%
  #drop geometry
  st_drop_geometry()%>%
  #display list of variables
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

slice_head(LondonWardsData, n=5)

# Create a new data frame just containing the two variables we are interested in
mydata <- LondonWards %>%
      st_drop_geometry()%>%
      dplyr::select(c(PctOwned20, PctNoEngli))

#– check variable distributions first
histplot <- ggplot(data=mydata, aes(x=PctOwned20))
histplot +geom_histogram()
histplot <- ggplot(data=mydata, aes(x= PctNoEngli))
histplot +geom_histogram()
```

Let's make our k-means find 3 clusters with 25 iterations. The graphics below by [Allison Horst](https://twitter.com/allison_horst) will help explain the process...

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_2.jpg')
```

```{r}
fit <- mydata %>%
  kmeans(., 3, nstart=25)
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_3.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_4.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_5.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_6.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_7.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_8.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_9.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_10.jpg')
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_11.jpg')
```

Now let's get out cluster means using `tidy()` from the `tidymodels` package. Tidy creates a tibble that summarizes the model.

Then plot them and then add them to our London wards...

```{r}
# get cluster means
library(tidymodels)

centroid <- tidy(fit)%>%
  #print the results of the cluster groupings
  print()%>%
  dplyr::select(PctOwned20, PctNoEngli)

# as we only have variable two dimensions we can plot the clusters on a graph
p <- ggplot(mydata,aes(PctOwned20, PctNoEngli))+
  geom_point(aes(colour=factor(fit$cluster)))+
  geom_point(data=centroid,aes(PctOwned20, PctNoEngli), size=7, shape=18)+ theme(legend.position="none")

LondonWards <- fit %>% 
  # 
  augment(., LondonWards)%>%
  dplyr::select(WD11CD, .cluster)%>%
  #make sure the .cluster column is numeric
  mutate(across(.cluster, as.numeric))%>%
  # join the .cluster to our sf layer
  left_join(LondonWards, 
            .,
            by = c("WD11CD" = "WD11CD"))


#now map our geodeomographic classification
map <- ggplot(LondonWards) + 
  geom_sf(mapping = aes(fill=.cluster))+
  scale_fill_continuous(breaks=c(1,2,3))
map
```

```{r echo=FALSE, out.width = "450pt", fig.align='center', cache=TRUE}
knitr::include_graphics('allisonhorst_images/kmeans_12.jpg')
```

Now of course this is just the most basic of classifications, but you can easily see how you could include more variables or different variables to create a different classification - this is perhaps something you could try. 

I haven't even gone into using different clustering algorithms, how to decide on the appropriate number of clusters, using silhoutte plots to assess the strength of the clusters or creating pen-portraits using the variable z-scores for each cluster - this is practically a whole course in its own right... or indeed a dissertation topic!
